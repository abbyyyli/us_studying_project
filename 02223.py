import requests
from bs4 import BeautifulSoup
import json
import csv
import os
import time
import random
import glob
from urllib.robotparser import RobotFileParser

# è¨­å®š User-Agent æ¸…å–®ï¼ˆéš¨æ©Ÿé¸æ“‡ï¼Œé¿å…è¢«å°é–ï¼‰
USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/537.36"
]

# è‡ªå‹•è®€å–æ‰€æœ‰ `url_*.txt` æª”æ¡ˆ
school_files = glob.glob("url_*.txt")
output_folder = "crawl_results"
os.makedirs(output_folder, exist_ok=True)

# æª¢æŸ¥ robots.txt æ˜¯å¦å…è¨±çˆ¬å–
def is_allowed_to_scrape(url):
    base_url = "/".join(url.split("/")[:3])
    robots_url = base_url + "/robots.txt"
    rp = RobotFileParser()
    rp.set_url(robots_url)
    try:
        rp.read()
        return rp.can_fetch("*", url)
    except:
        return True  # å¦‚æœ robots.txt ç„¡æ³•è®€å–ï¼Œé è¨­å…è¨±çˆ¬å–

# é€å€‹å­¸æ ¡è™•ç†
for school_file in school_files:
    school_name = school_file.replace("url_", "").replace(".txt", "")  # å–å¾—å­¸æ ¡åç¨±
    school_folder = os.path.join(output_folder, school_name)
    os.makedirs(school_folder, exist_ok=True)

    print(f"\nğŸ”„ é–‹å§‹çˆ¬å– {school_name} çš„è³‡æ–™...")

    # è®€å–è©²å­¸æ ¡çš„ URL
    with open(school_file, "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f.readlines() if line.strip()]

    all_data = []  # å­˜æ”¾çˆ¬å–çµæœ

    # éæ­· URL é€²è¡Œçˆ¬å–
    for url in urls:
        print(f"ğŸŒ æ­£åœ¨çˆ¬å– {url}...")

        # æª¢æŸ¥ robots.txt æ˜¯å¦å…è¨±çˆ¬å–
        if not is_allowed_to_scrape(url):
            print(f"ğŸš« {url} è¢« robots.txt é™åˆ¶ï¼Œè·³é")
            continue

        headers = {"User-Agent": random.choice(USER_AGENTS)}

        try:
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()  # è‹¥ HTTP å›æ‡‰éŒ¯èª¤ (å¦‚ 404)ï¼Œæœƒè§¸ç™¼ Exception
        except requests.exceptions.RequestException as e:
            print(f"âŒ ç„¡æ³•çˆ¬å– {url}: {e}")
            time.sleep(5)
            continue

        soup = BeautifulSoup(response.text, "html.parser")
        main_content = soup.find("main", id="main-content")

        if main_content:
            # 1ï¸âƒ£ æå–æ¨™é¡Œï¼ˆh1, h2, h3ï¼‰
            headings = [h.get_text(strip=True) for h in main_content.find_all(["h1", "h2", "h3"])]

            # 2ï¸âƒ£ æå–æ®µè½å…§å®¹ï¼ˆéæ¿¾æ‰ "Explore"ï¼‰
            paragraphs = [p.get_text(strip=True) for p in main_content.find_all("p") if "Explore" not in p.get_text()]

            # 3ï¸âƒ£ æå–æ‰€æœ‰è¶…é€£çµï¼ˆå»é™¤é‡è¤‡ï¼‰
            links = list(set(a["href"] for a in main_content.find_all("a", href=True)))

            # 4ï¸âƒ£ æ•´ç†æ•¸æ“š
            page_data = {
                "url": url,
                "headings": headings,
                "content": paragraphs,
                "links": links
            }
            all_data.append(page_data)

            print(f"âœ… æˆåŠŸçˆ¬å– {url}ï¼Œæ¨™é¡Œ {len(headings)}ï¼Œæ®µè½ {len(paragraphs)}ï¼Œé€£çµ {len(links)}")
        else:
            print(f"âš ï¸ {url} æ²’æœ‰ä¸»è¦å…§å®¹å€å¡Šï¼Œè·³é")

        # **éš¨æ©Ÿç­‰å¾… 1-3 ç§’ï¼Œé¿å…è«‹æ±‚éå¿«è¢«å°é–**
        time.sleep(random.uniform(1, 3))

    # å„²å­˜çˆ¬å–çµæœ
    txt_file = os.path.join(school_folder, f"{school_name}.txt")
    json_file = os.path.join(school_folder, f"{school_name}.json")
    csv_file = os.path.join(school_folder, f"{school_name}.csv")

    # 5ï¸âƒ£ å­˜æˆ TXT æª”æ¡ˆ
    with open(txt_file, "w", encoding="utf-8") as txt_out:
        for page in all_data:
            txt_out.write(f"\n===== {page['url']} =====\n")
            txt_out.write("\n".join(page["headings"]) + "\n\n")
            txt_out.write("\n".join(page["content"]) + "\n\n")
            txt_out.write("Links:\n" + "\n".join(page["links"]) + "\n")

    print(f"ğŸ“‚ {school_name} çš„ TXT å­˜å…¥ {txt_file}ï¼")

    # 6ï¸âƒ£ å­˜æˆ JSON æª”æ¡ˆ
    with open(json_file, "w", encoding="utf-8") as json_out:
        json.dump(all_data, json_out, ensure_ascii=False, indent=4)

    print(f"ğŸ“‚ {school_name} çš„ JSON å­˜å…¥ {json_file}ï¼")

    # 7ï¸âƒ£ å­˜æˆ CSV æª”æ¡ˆ
    with open(csv_file, "w", newline="", encoding="utf-8") as csv_out:
        writer = csv.writer(csv_out)
        writer.writerow(["URL", "Type", "Content"])  # æ¨™é¡Œåˆ—

        for page in all_data:
            for h in page["headings"]:
                writer.writerow([page["url"], "Heading", h])
            for p in page["content"]:
                writer.writerow([page["url"], "Paragraph", p])
            for l in page["links"]:
                writer.writerow([page["url"], "Link", l])

    print(f"ğŸ“‚ {school_name} çš„ CSV å­˜å…¥ {csv_file}ï¼")

print("ğŸš€ æ‰€æœ‰å­¸æ ¡çš„çˆ¬å–å·¥ä½œå®Œæˆï¼")
